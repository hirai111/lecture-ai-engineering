import streamlit as st
import ui
import llm
import database
import metrics
import data
import torch
from transformers import pipeline
from config import MODEL_NAME
from huggingface_hub import HfFolder

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š ---
st.set_page_config(page_title="Gemma Chatbot", layout="wide")

# --- ç°¡æ˜“çš„ãªã‚«ã‚¹ã‚¿ãƒ CSS ---
st.markdown("""
<style>
    /* ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 0.8rem;
        display: flex;
    }
    .user-message {
        background-color: #F0F4F9;
        margin-left: auto;
        max-width: 80%;
    }
    .ai-message {
        background-color: #6C63FF;
        color: white;
        margin-right: auto;
        max-width: 80%;
    }
    /* ãƒœã‚¿ãƒ³ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stButton>button {
        border-radius: 0.5rem;
    }
    /* ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    h1 {
        color: #6C63FF;
    }
</style>
""", unsafe_allow_html=True)

# --- åˆæœŸåŒ–å‡¦ç† ---
metrics.initialize_nltk()
database.init_db()
data.ensure_initial_data()

# --- ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ ---
@st.cache_resource
def load_model():
    """LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        st.info(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
        pipe = pipeline(
            "text-generation",
            model=MODEL_NAME,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device=device
        )
        st.success(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return pipe
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.error("GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return None

pipe = llm.load_model()

# --- ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜ ---
st.title("ğŸ¤– Gemma ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
st.write("Gemmaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚å›ç­”ã«å¯¾ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã§ãã¾ã™ã€‚")
st.markdown("---")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
st.sidebar.title("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'page' not in st.session_state:
    st.session_state.page = "ãƒãƒ£ãƒƒãƒˆ"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒšãƒ¼ã‚¸

# ãƒšãƒ¼ã‚¸é¸æŠ
page = st.sidebar.radio(
    "ãƒšãƒ¼ã‚¸é¸æŠ",
    ["ãƒãƒ£ãƒƒãƒˆ", "å±¥æ­´é–²è¦§", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†"],
    key="page_selector",
    index=["ãƒãƒ£ãƒƒãƒˆ", "å±¥æ­´é–²è¦§", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†"].index(st.session_state.page),
    on_change=lambda: setattr(st.session_state, 'page', st.session_state.page_selector)
)

# ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
st.sidebar.markdown("---")
st.sidebar.subheader("ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
st.sidebar.markdown(f"**ãƒ¢ãƒ‡ãƒ«å**: {MODEL_NAME}")
st.sidebar.markdown(f"**å®Ÿè¡Œç’°å¢ƒ**: {'GPU' if torch.cuda.is_available() else 'CPU'}")

# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
if st.session_state.page == "ãƒãƒ£ãƒƒãƒˆ":
    if pipe:
        # æ³¨æ„ï¼å¼•æ•°ã®é †åºãŒæ­£ã—ã„ã“ã¨ã‚’ç¢ºèª (ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ)
        ui.display_chat_page(pipe)
    else:
        st.error("ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
elif st.session_state.page == "å±¥æ­´é–²è¦§":
    # å±¥æ­´ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹å‰ã«ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚ˆã†ui.pyã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã‚’æƒ³å®š
    ui.display_history_page()
elif st.session_state.page == "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†":
    ui.display_data_page()

# --- ãƒ•ãƒƒã‚¿ãƒ¼ ---
st.sidebar.markdown("---")
st.sidebar.info("Gemmaã¯ã€Google DeepMindã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚")